{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8191d70",
   "metadata": {},
   "source": [
    "## 1- Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63de3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0444d28",
   "metadata": {},
   "source": [
    "## 2- Handle french dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c588c44-5680-40d2-9769-82144f1843be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Columns: Index(['sentence1', 'sentence2', 'score', 'dataset'], dtype='object')\n",
      "French Parquet loaded. Extracted 24454 sentences.\n",
      "French Text loaded. Extracted 50000 sentences.\n",
      "Sample French Data:\n",
      "                                                     text  language\n",
      "49995     Comment vous écrivez sueur en langue chaouie ?  Français\n",
      "49996     Comment il doit écrire mot en langue hybride ?  Français\n",
      "49997  Je me suis perdu parce que je n'avais pas de c...  Français\n",
      "49998                 Elle n'a pas déménagé à Guerrouma.  Français\n",
      "49999                             Tom est prêt pour toi.  Français\n"
     ]
    }
   ],
   "source": [
    "# Rename this list to be clear it holds tables, not raw text rows\n",
    "dataframes_list = []\n",
    "\n",
    "# --- Load French Data ---\n",
    "try:\n",
    "    df_parquet = pd.read_parquet('../data/raw/fr_train.parquet', engine='pyarrow')\n",
    "    print(\"Parquet Columns:\", df_parquet.columns)\n",
    "\n",
    "    # 1. Extract sentence1 and sentence2\n",
    "    part1 = df_parquet[['sentence1']].rename(columns={'sentence1': 'text'})\n",
    "    part2 = df_parquet[['sentence2']].rename(columns={'sentence2': 'text'})\n",
    "\n",
    "    # 2. Combine them\n",
    "    df_clean_parquet = pd.concat([part1, part2], ignore_index=True)\n",
    "    df_clean_parquet=df_clean_parquet[:50000]  # Limit to first 50,000 rows\n",
    "    # 3. CRITICAL FIX: Label this as 'Français' (since filename is fr_train)\n",
    "    df_clean_parquet['language'] = 'Français'\n",
    "\n",
    "    # 4. Drop empty rows\n",
    "    df_clean_parquet.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "    # 5. Append the DATAFRAME to the list\n",
    "    dataframes_list.append(df_clean_parquet)\n",
    "    print(f\"French Parquet loaded. Extracted {len(df_clean_parquet)} sentences.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet: {e}\")\n",
    "\n",
    "french_data = []\n",
    "try:\n",
    "    with open('../data/raw/fr.txt', 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 50_000:\n",
    "                break\n",
    "            if line.strip():\n",
    "                french_data.append({'text': line.strip(), 'language': 'Français'})\n",
    "    \n",
    "    df_txt = pd.DataFrame(french_data)\n",
    "    dataframes_list.append(df_txt)\n",
    "    print(f\"French Text loaded. Extracted {len(df_txt)} sentences.\")\n",
    "    print(\"Sample French Data:\\n\", df_txt.tail())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TXT: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa1001",
   "metadata": {},
   "source": [
    "## 3-Handle english dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035fddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text loaded. Extracted 50000 sentences.\n",
      "Sample English Data:\n",
      "                                                     text language\n",
      "49995                          Yanni had a Berber class.  English\n",
      "49996  Skura never encouraged or acknowledged Yanni's...  English\n",
      "49997          I posted this picture on my social media.  English\n",
      "49998                          \"Can I have your number?\"  English\n",
      "49999                                             \"Why?\"  English\n",
      "Error loading JSONL: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "# --- Load English Data ---\n",
    "english_data = []\n",
    "try:\n",
    "    with open('../data/raw/en.txt', 'r', encoding='utf-8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i >= 50_000:\n",
    "                break\n",
    "            if line.strip():\n",
    "                english_data.append({'text': line.strip(), 'language': 'English'})\n",
    "    \n",
    "    df_txt = pd.DataFrame(english_data)\n",
    "    dataframes_list.append(df_txt)\n",
    "    print(f\"English Text loaded. Extracted {len(df_txt)} sentences.\")\n",
    "    print(\"Sample English Data:\\n\", df_txt.tail())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading TXT: {e}\")\n",
    "\n",
    "jsonl_data = []\n",
    "\n",
    "try:\n",
    "    with open('../data/raw/train_en.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for i,line in f:\n",
    "            if i >= 50_000:\n",
    "                break\n",
    "            if line.strip(): # Check if line is not empty\n",
    "                # 1. Parse the JSON line into a Python dictionary\n",
    "                json_obj = json.loads(line)\n",
    "                \n",
    "                # 2. Extract the text. \n",
    "                if 'sentence' in json_obj:\n",
    "                    text_value = json_obj['sentence']\n",
    "                    \n",
    "                    # 3. Append to your list\n",
    "                    jsonl_data.append({'text': text_value, 'language': 'English'})\n",
    "    \n",
    "    # 4. Convert to DataFrame and append to your main list\n",
    "    df_jsonl = pd.DataFrame(jsonl_data)\n",
    "    dataframes_list.append(df_jsonl)\n",
    "    \n",
    "    print(f\"JSONL loaded. Extracted {len(df_jsonl)} sentences.\")\n",
    "    print(\"Sample JSONL Data:\\n\", df_jsonl.tail())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSONL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42710a65",
   "metadata": {},
   "source": [
    "## 4- Handle Darija dataframes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d76520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 loaded: 48840 rows.\n",
      "File 2 loaded: 10020 rows.\n",
      "File 3 loaded: 50000 rows.\n",
      "Parquet loaded: 850 rows.\n",
      "\n",
      "--------------------------------------\n",
      "Total Darija Loaded: 109710 sentences\n",
      "Sample:\n",
      "                                                      text language\n",
      "109705  hir thenna, 3amrak t9dar t9na3ni anna Raja 7se...   Darija\n",
      "109706  hahaha rak ma3arf walo flkora, tatkon l3bti 9e...   Darija\n",
      "109707  mafkertich tdir chi mchro3 asahbi, malk m3gaz,...   Darija\n",
      "109708  mcha l sou9 it9eda, fach irje3 ghadi ngolha li...   Darija\n",
      "109709  nti hi goliha liha o machi choghlek f ach ghad...   Darija\n"
     ]
    }
   ],
   "source": [
    "# List to hold all Darija data chunks\n",
    "darija_chunks = []\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Load 'darija_sentences.csv'\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    # Assuming this file has no header and is just lines of text\n",
    "    # header=None ensures we don't lose the first sentence\n",
    "    # names=['text'] names the column immediately\n",
    "    df1 = pd.read_csv('../data/raw/darija_sentences.csv', nrows=50000, header=None, names=['text'], encoding='utf-8')\n",
    "    df1['language'] = 'Darija'\n",
    "    darija_chunks.append(df1)\n",
    "    print(f\"File 1 loaded: {len(df1)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading File 1: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Load 'sentences_darija2.csv' (FIXED LOGIC)\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df2 = pd.read_csv('../data/raw/sentences_darija2.csv', nrows=50000)\n",
    "    \n",
    "    # STRATEGY: We don't know the column name, so we grab the first one.\n",
    "    first_col = df2.columns[0] \n",
    "    df2 = df2.rename(columns={first_col: 'text'})\n",
    "    \n",
    "    # Clean and Label\n",
    "    df2['language'] = 'Darija'\n",
    "    df2 = df2[['text', 'language']] # Keep only relevant columns\n",
    "    \n",
    "    darija_chunks.append(df2)\n",
    "    print(f\"File 2 loaded: {len(df2)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading File 2: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Load 'train_darija.csv'\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df3 = pd.read_csv('../data/raw/train_darija.csv', nrows=50000)\n",
    "    \n",
    "    if \"sentence\" in df3.columns:\n",
    "        df3 = df3.rename(columns={'sentence': 'text'})\n",
    "        df3['language'] = 'Darija'\n",
    "        df3 = df3[['text', 'language']]\n",
    "        darija_chunks.append(df3)\n",
    "        print(f\"File 3 loaded: {len(df3)} rows.\")\n",
    "    else:\n",
    "        print(\"File 3 skipped: Column 'sentence' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading File 3: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Load Parquet\n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    df_parquet = pd.read_parquet('../data/raw/darija_sentences_3.parquet', engine='pyarrow')\n",
    "    \n",
    "    # Select and Rename\n",
    "    if 'Arabizi' in df_parquet.columns:\n",
    "        df_parquet = df_parquet[['Arabizi']].rename(columns={'Arabizi': 'text'})\n",
    "        df_parquet['language'] = 'Darija'\n",
    "        df_parquet = df_parquet.head(50000) # Limit to 50k\n",
    "        \n",
    "        darija_chunks.append(df_parquet)\n",
    "        print(f\"Parquet loaded: {len(df_parquet)} rows.\")\n",
    "    else:\n",
    "        print(\"Parquet skipped: Column 'Arabizi' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FINAL MERGE\n",
    "# ---------------------------------------------------------\n",
    "if darija_chunks:\n",
    "    # Combine all the chunks into one Darija DataFrame\n",
    "    df_final_darija = pd.concat(darija_chunks, ignore_index=True)\n",
    "    \n",
    "    # Drop any empty rows just in case\n",
    "    df_final_darija.dropna(subset=['text'], inplace=True)\n",
    "    \n",
    "    # Append to your MAIN list (that holds English/French too)\n",
    "    dataframes_list.append(df_final_darija)\n",
    "    \n",
    "    print(\"\\n--------------------------------------\")\n",
    "    print(f\"Total Darija Loaded: {len(df_final_darija)} sentences\")\n",
    "    print(\"Sample:\\n\", df_final_darija.tail())\n",
    "else:\n",
    "    print(\"No Darija data was loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa1e118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Dataset Shape: (234164, 2)\n",
      "                                                     text language\n",
      "234159  hir thenna, 3amrak t9dar t9na3ni anna Raja 7se...   Darija\n",
      "234160  hahaha rak ma3arf walo flkora, tatkon l3bti 9e...   Darija\n",
      "234161  mafkertich tdir chi mchro3 asahbi, malk m3gaz,...   Darija\n",
      "234162  mcha l sou9 it9eda, fach irje3 ghadi ngolha li...   Darija\n",
      "234163  nti hi goliha liha o machi choghlek f ach ghad...   Darija\n",
      "['Français' 'English' 'Darija']\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL MERGE ---\n",
    "if dataframes_list:\n",
    "    df = pd.concat(dataframes_list, ignore_index=True)\n",
    "    print(\"\\nFinal Dataset Shape:\", df.shape)\n",
    "    print(df.tail())\n",
    "    print(df[\"language\"].unique())\n",
    "else:\n",
    "    print(\"No data loaded.\")\n",
    "\n",
    "# Francais : 600 000 sentences\n",
    "# English : 1 850 000 sentences\n",
    "# Darija : 1 300 000 sentences\n",
    "# total=2 500 000 sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50278a",
   "metadata": {},
   "source": [
    "## Convert dataset to csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e379175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/combined_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
